{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13fedd0-79aa-44f1-9c6c-f0be675f4e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns: ['sample_id', 'catalog_content', 'image_link', 'price']\n",
      "Test columns: ['sample_id', 'catalog_content', 'image_link']\n",
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
      "3      55858  Item Name: Judeeâ€™s Blue Cheese Powder 11.25 oz...   \n",
      "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
      "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
      "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  \n",
      "   sample_id                                    catalog_content  \\\n",
      "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
      "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
      "2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n",
      "3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Oz...   \n",
      "4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n",
      "\n",
      "                                          image_link  \n",
      "0  https://m.media-amazon.com/images/I/71hoAn78AW...  \n",
      "1  https://m.media-amazon.com/images/I/61ex8NHCIj...  \n",
      "2  https://m.media-amazon.com/images/I/61KCM61J8e...  \n",
      "3  https://m.media-amazon.com/images/I/51Ex6uOH7y...  \n",
      "4  https://m.media-amazon.com/images/I/71QYlrOMoS...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"C:/Users/pm5cd/Downloads/68e8d1d70b66d_student_resource/student_resource/dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"C:/Users/pm5cd/Downloads/68e8d1d70b66d_student_resource/student_resource/dataset/test.csv\")\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"Train columns:\", train_df.columns.tolist())\n",
    "print(\"Test columns:\", test_df.columns.tolist())\n",
    "\n",
    "# Preview data\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3edb7eae-e9d8-4552-9f32-896ee232bdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM fold 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttraining's l1: 0.523731\tvalid_1's l1: 0.591136\n",
      "[400]\ttraining's l1: 0.469814\tvalid_1's l1: 0.574228\n",
      "[600]\ttraining's l1: 0.43609\tvalid_1's l1: 0.566999\n",
      "[800]\ttraining's l1: 0.411979\tvalid_1's l1: 0.563157\n",
      "[1000]\ttraining's l1: 0.393489\tvalid_1's l1: 0.560521\n",
      "[1200]\ttraining's l1: 0.378902\tvalid_1's l1: 0.558563\n",
      "[1400]\ttraining's l1: 0.366565\tvalid_1's l1: 0.556958\n",
      "[1600]\ttraining's l1: 0.356163\tvalid_1's l1: 0.555895\n",
      "[1800]\ttraining's l1: 0.347214\tvalid_1's l1: 0.554724\n",
      "[2000]\ttraining's l1: 0.339169\tvalid_1's l1: 0.553695\n",
      "[2200]\ttraining's l1: 0.332201\tvalid_1's l1: 0.552824\n",
      "[2400]\ttraining's l1: 0.326554\tvalid_1's l1: 0.552043\n",
      "[2600]\ttraining's l1: 0.321313\tvalid_1's l1: 0.551502\n",
      "[2800]\ttraining's l1: 0.316396\tvalid_1's l1: 0.550898\n",
      "[3000]\ttraining's l1: 0.311822\tvalid_1's l1: 0.5504\n",
      "[3200]\ttraining's l1: 0.307756\tvalid_1's l1: 0.549984\n",
      "[3400]\ttraining's l1: 0.303913\tvalid_1's l1: 0.549636\n",
      "[3600]\ttraining's l1: 0.300611\tvalid_1's l1: 0.549373\n",
      "[3800]\ttraining's l1: 0.297288\tvalid_1's l1: 0.54915\n",
      "[4000]\ttraining's l1: 0.293938\tvalid_1's l1: 0.548867\n",
      "[4200]\ttraining's l1: 0.290672\tvalid_1's l1: 0.548517\n",
      "[4400]\ttraining's l1: 0.287691\tvalid_1's l1: 0.548204\n",
      "[4600]\ttraining's l1: 0.28481\tvalid_1's l1: 0.548018\n",
      "[4800]\ttraining's l1: 0.282341\tvalid_1's l1: 0.547781\n",
      "[5000]\ttraining's l1: 0.279818\tvalid_1's l1: 0.547634\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4998]\ttraining's l1: 0.279849\tvalid_1's l1: 0.547631\n",
      "Training LightGBM fold 2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttraining's l1: 0.525741\tvalid_1's l1: 0.582983\n",
      "[400]\ttraining's l1: 0.471266\tvalid_1's l1: 0.5648\n",
      "[600]\ttraining's l1: 0.437846\tvalid_1's l1: 0.557419\n",
      "[800]\ttraining's l1: 0.413704\tvalid_1's l1: 0.553227\n",
      "[1000]\ttraining's l1: 0.394709\tvalid_1's l1: 0.550521\n",
      "[1200]\ttraining's l1: 0.380001\tvalid_1's l1: 0.548703\n",
      "[1400]\ttraining's l1: 0.36782\tvalid_1's l1: 0.547144\n",
      "[1600]\ttraining's l1: 0.357939\tvalid_1's l1: 0.545928\n",
      "[1800]\ttraining's l1: 0.349184\tvalid_1's l1: 0.544991\n",
      "[2000]\ttraining's l1: 0.341718\tvalid_1's l1: 0.544239\n",
      "[2200]\ttraining's l1: 0.334912\tvalid_1's l1: 0.54365\n",
      "[2400]\ttraining's l1: 0.328376\tvalid_1's l1: 0.542858\n",
      "[2600]\ttraining's l1: 0.32273\tvalid_1's l1: 0.542274\n",
      "[2800]\ttraining's l1: 0.317757\tvalid_1's l1: 0.541748\n",
      "[3000]\ttraining's l1: 0.312931\tvalid_1's l1: 0.541264\n",
      "[3200]\ttraining's l1: 0.308569\tvalid_1's l1: 0.540846\n",
      "[3400]\ttraining's l1: 0.304646\tvalid_1's l1: 0.540424\n",
      "[3600]\ttraining's l1: 0.301164\tvalid_1's l1: 0.539946\n",
      "[3800]\ttraining's l1: 0.297741\tvalid_1's l1: 0.53965\n",
      "[4000]\ttraining's l1: 0.294478\tvalid_1's l1: 0.539358\n",
      "[4200]\ttraining's l1: 0.291231\tvalid_1's l1: 0.539094\n",
      "[4400]\ttraining's l1: 0.288262\tvalid_1's l1: 0.53887\n",
      "[4600]\ttraining's l1: 0.285702\tvalid_1's l1: 0.538671\n",
      "[4800]\ttraining's l1: 0.283251\tvalid_1's l1: 0.53844\n",
      "[5000]\ttraining's l1: 0.28096\tvalid_1's l1: 0.53824\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4993]\ttraining's l1: 0.281052\tvalid_1's l1: 0.538235\n",
      "Training LightGBM fold 3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttraining's l1: 0.526001\tvalid_1's l1: 0.58245\n",
      "[400]\ttraining's l1: 0.472053\tvalid_1's l1: 0.564627\n",
      "[600]\ttraining's l1: 0.438511\tvalid_1's l1: 0.557246\n",
      "[800]\ttraining's l1: 0.414018\tvalid_1's l1: 0.553074\n",
      "[1000]\ttraining's l1: 0.395314\tvalid_1's l1: 0.550419\n",
      "[1200]\ttraining's l1: 0.380618\tvalid_1's l1: 0.548278\n",
      "[1400]\ttraining's l1: 0.367965\tvalid_1's l1: 0.546642\n",
      "[1600]\ttraining's l1: 0.357414\tvalid_1's l1: 0.545317\n",
      "[1800]\ttraining's l1: 0.348023\tvalid_1's l1: 0.544369\n",
      "[2000]\ttraining's l1: 0.340267\tvalid_1's l1: 0.543493\n",
      "[2200]\ttraining's l1: 0.33351\tvalid_1's l1: 0.5428\n",
      "[2400]\ttraining's l1: 0.327627\tvalid_1's l1: 0.542146\n",
      "[2600]\ttraining's l1: 0.322283\tvalid_1's l1: 0.541688\n",
      "[2800]\ttraining's l1: 0.317253\tvalid_1's l1: 0.541159\n",
      "[3000]\ttraining's l1: 0.312578\tvalid_1's l1: 0.540764\n",
      "[3200]\ttraining's l1: 0.308589\tvalid_1's l1: 0.540369\n",
      "[3400]\ttraining's l1: 0.304672\tvalid_1's l1: 0.539985\n",
      "[3600]\ttraining's l1: 0.300884\tvalid_1's l1: 0.539623\n",
      "[3800]\ttraining's l1: 0.297543\tvalid_1's l1: 0.539343\n",
      "[4000]\ttraining's l1: 0.294222\tvalid_1's l1: 0.539041\n",
      "[4200]\ttraining's l1: 0.291263\tvalid_1's l1: 0.538781\n",
      "[4400]\ttraining's l1: 0.288159\tvalid_1's l1: 0.538487\n",
      "[4600]\ttraining's l1: 0.285368\tvalid_1's l1: 0.538262\n",
      "[4800]\ttraining's l1: 0.282845\tvalid_1's l1: 0.538026\n",
      "[5000]\ttraining's l1: 0.280641\tvalid_1's l1: 0.537821\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\ttraining's l1: 0.280648\tvalid_1's l1: 0.537821\n",
      "Training LightGBM fold 4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttraining's l1: 0.528823\tvalid_1's l1: 0.571895\n",
      "[400]\ttraining's l1: 0.474104\tvalid_1's l1: 0.554119\n",
      "[600]\ttraining's l1: 0.439773\tvalid_1's l1: 0.546984\n",
      "[800]\ttraining's l1: 0.415309\tvalid_1's l1: 0.543051\n",
      "[1000]\ttraining's l1: 0.396439\tvalid_1's l1: 0.540382\n",
      "[1200]\ttraining's l1: 0.381428\tvalid_1's l1: 0.538447\n",
      "[1400]\ttraining's l1: 0.368853\tvalid_1's l1: 0.53706\n",
      "[1600]\ttraining's l1: 0.358409\tvalid_1's l1: 0.535883\n",
      "[1800]\ttraining's l1: 0.349511\tvalid_1's l1: 0.534985\n",
      "[2000]\ttraining's l1: 0.341743\tvalid_1's l1: 0.534365\n",
      "[2200]\ttraining's l1: 0.335021\tvalid_1's l1: 0.533797\n",
      "[2400]\ttraining's l1: 0.329135\tvalid_1's l1: 0.53321\n",
      "[2600]\ttraining's l1: 0.3236\tvalid_1's l1: 0.532776\n",
      "[2800]\ttraining's l1: 0.318727\tvalid_1's l1: 0.532346\n",
      "[3000]\ttraining's l1: 0.314309\tvalid_1's l1: 0.53193\n",
      "[3200]\ttraining's l1: 0.310025\tvalid_1's l1: 0.531478\n",
      "[3400]\ttraining's l1: 0.30542\tvalid_1's l1: 0.531048\n",
      "[3600]\ttraining's l1: 0.301332\tvalid_1's l1: 0.530683\n",
      "[3800]\ttraining's l1: 0.29805\tvalid_1's l1: 0.530376\n",
      "[4000]\ttraining's l1: 0.295032\tvalid_1's l1: 0.530088\n",
      "[4200]\ttraining's l1: 0.292163\tvalid_1's l1: 0.529808\n",
      "[4400]\ttraining's l1: 0.289553\tvalid_1's l1: 0.529599\n",
      "[4600]\ttraining's l1: 0.286744\tvalid_1's l1: 0.529327\n",
      "[4800]\ttraining's l1: 0.28407\tvalid_1's l1: 0.5292\n",
      "[5000]\ttraining's l1: 0.281737\tvalid_1's l1: 0.528959\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.281737\tvalid_1's l1: 0.528959\n",
      "Training LightGBM fold 5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[200]\ttraining's l1: 0.5255\tvalid_1's l1: 0.585963\n",
      "[400]\ttraining's l1: 0.470873\tvalid_1's l1: 0.568068\n",
      "[600]\ttraining's l1: 0.437159\tvalid_1's l1: 0.560756\n",
      "[800]\ttraining's l1: 0.412908\tvalid_1's l1: 0.556543\n",
      "[1000]\ttraining's l1: 0.394155\tvalid_1's l1: 0.553682\n",
      "[1200]\ttraining's l1: 0.379002\tvalid_1's l1: 0.551622\n",
      "[1400]\ttraining's l1: 0.366499\tvalid_1's l1: 0.549987\n",
      "[1600]\ttraining's l1: 0.356335\tvalid_1's l1: 0.548801\n",
      "[1800]\ttraining's l1: 0.347389\tvalid_1's l1: 0.547797\n",
      "[2000]\ttraining's l1: 0.339745\tvalid_1's l1: 0.546932\n",
      "[2200]\ttraining's l1: 0.332784\tvalid_1's l1: 0.546256\n",
      "[2400]\ttraining's l1: 0.326792\tvalid_1's l1: 0.545685\n",
      "[2600]\ttraining's l1: 0.321491\tvalid_1's l1: 0.545087\n",
      "[2800]\ttraining's l1: 0.316191\tvalid_1's l1: 0.54453\n",
      "[3000]\ttraining's l1: 0.311546\tvalid_1's l1: 0.544096\n",
      "[3200]\ttraining's l1: 0.307488\tvalid_1's l1: 0.543713\n",
      "[3400]\ttraining's l1: 0.303305\tvalid_1's l1: 0.543287\n",
      "[3600]\ttraining's l1: 0.29923\tvalid_1's l1: 0.5428\n",
      "[3800]\ttraining's l1: 0.295811\tvalid_1's l1: 0.542552\n",
      "[4000]\ttraining's l1: 0.291762\tvalid_1's l1: 0.542279\n",
      "[4200]\ttraining's l1: 0.28796\tvalid_1's l1: 0.541924\n",
      "[4400]\ttraining's l1: 0.285034\tvalid_1's l1: 0.541709\n",
      "[4600]\ttraining's l1: 0.282509\tvalid_1's l1: 0.541519\n",
      "[4800]\ttraining's l1: 0.28038\tvalid_1's l1: 0.541338\n",
      "[5000]\ttraining's l1: 0.277835\tvalid_1's l1: 0.541103\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.277835\tvalid_1's l1: 0.541103\n",
      "Training CatBoost fold 1\n",
      "Training CatBoost fold 2\n",
      "Training CatBoost fold 3\n",
      "Training CatBoost fold 4\n",
      "Training CatBoost fold 5\n",
      "Training XGBoost fold 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 142\u001b[0m\n\u001b[0;32m    139\u001b[0m     y_tr, y_val \u001b[38;5;241m=\u001b[39m y_train[tr_idx], y_train[val_idx]\n\u001b[0;32m    141\u001b[0m     model \u001b[38;5;241m=\u001b[39m XGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params)\n\u001b[1;32m--> 142\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_tr, y_tr, eval_set\u001b[38;5;241m=\u001b[39m[(X_val, y_val)], early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    144\u001b[0m     pred_xgb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m/\u001b[39m kf\u001b[38;5;241m.\u001b[39mn_splits\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Ensemble\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "# -----------------------------\n",
    "# Import libraries\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# Load Dataset\n",
    "# -----------------------------\n",
    "train_df = pd.read_csv(r\"C:/Users/pm5cd/Downloads/68e8d1d70b66d_student_resource/student_resource/dataset/train.csv\")\n",
    "test_df = pd.read_csv(r\"C:/Users/pm5cd/Downloads/68e8d1d70b66d_student_resource/student_resource/dataset/test.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# Feature Engineering from catalog_content\n",
    "# -----------------------------\n",
    "text_column = 'catalog_content'\n",
    "for df in [train_df, test_df]:\n",
    "    df['content_length'] = df[text_column].apply(lambda x: len(str(x)))\n",
    "    df['word_count'] = df[text_column].apply(lambda x: len(str(x).split()))\n",
    "    df['digit_count'] = df[text_column].apply(lambda x: sum(c.isdigit() for c in str(x)))\n",
    "    df['avg_word_length'] = df['content_length'] / (df['word_count'] + 1)\n",
    "    df['digit_ratio'] = df['digit_count'] / (df['content_length'] + 1)\n",
    "    df['caps_ratio'] = df[text_column].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / (len(str(x)) + 1))\n",
    "\n",
    "# -----------------------------\n",
    "# TF-IDF + TruncatedSVD\n",
    "# -----------------------------\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=8000,\n",
    "    ngram_range=(1,3),\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "tfidf_train = tfidf.fit_transform(train_df[text_column])\n",
    "tfidf_test = tfidf.transform(test_df[text_column])\n",
    "\n",
    "svd = TruncatedSVD(n_components=150, random_state=42)\n",
    "svd_train = svd.fit_transform(tfidf_train)\n",
    "svd_test = svd.transform(tfidf_test)\n",
    "\n",
    "# -----------------------------\n",
    "# Numeric Features\n",
    "# -----------------------------\n",
    "numeric_features = ['content_length','word_count','digit_count','avg_word_length','digit_ratio','caps_ratio']\n",
    "X_train_numeric = train_df[numeric_features].values\n",
    "X_test_numeric = test_df[numeric_features].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric = scaler.fit_transform(X_train_numeric)\n",
    "X_test_numeric = scaler.transform(X_test_numeric)\n",
    "\n",
    "# Combine numeric + SVD features\n",
    "X_train = np.hstack([X_train_numeric, svd_train])\n",
    "X_test = np.hstack([X_test_numeric, svd_test])\n",
    "\n",
    "# -----------------------------\n",
    "# Target\n",
    "# -----------------------------\n",
    "y_train = np.log1p(train_df['price'].values)\n",
    "\n",
    "# -----------------------------\n",
    "# K-Fold Setup\n",
    "# -----------------------------\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "pred_lgb = np.zeros(len(test_df))\n",
    "pred_cat = np.zeros(len(test_df))\n",
    "pred_xgb = np.zeros(len(test_df))\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Training LightGBM fold {fold+1}\")\n",
    "    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params=lgb_params,\n",
    "        train_set=train_data,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=200)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pred_lgb += model.predict(X_test, num_iteration=model.best_iteration) / kf.n_splits\n",
    "\n",
    "# -----------------------------\n",
    "# CatBoost Training\n",
    "# -----------------------------\n",
    "cat_params = {\n",
    "    'iterations':3000,\n",
    "    'learning_rate':0.02,\n",
    "    'depth':8,\n",
    "    'loss_function':'MAE',\n",
    "    'verbose':200,\n",
    "    'random_seed':42\n",
    "}\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Training CatBoost fold {fold+1}\")\n",
    "    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "    \n",
    "    model = CatBoostRegressor(**cat_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "    pred_cat += model.predict(X_test) / kf.n_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20d1ffc7-619d-4021-9297-eb22f405d2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost fold 1\n",
      "[0]\ttrain-rmse:0.93473\tval-rmse:0.94771\n",
      "[200]\ttrain-rmse:0.65130\tval-rmse:0.76070\n",
      "[400]\ttrain-rmse:0.55549\tval-rmse:0.74075\n",
      "[600]\ttrain-rmse:0.48918\tval-rmse:0.73283\n",
      "[800]\ttrain-rmse:0.43662\tval-rmse:0.72806\n",
      "[1000]\ttrain-rmse:0.39098\tval-rmse:0.72498\n",
      "[1200]\ttrain-rmse:0.35111\tval-rmse:0.72272\n",
      "[1400]\ttrain-rmse:0.31503\tval-rmse:0.72130\n",
      "[1600]\ttrain-rmse:0.28221\tval-rmse:0.71975\n",
      "[1800]\ttrain-rmse:0.25334\tval-rmse:0.71891\n",
      "[2000]\ttrain-rmse:0.22853\tval-rmse:0.71814\n",
      "[2200]\ttrain-rmse:0.20502\tval-rmse:0.71732\n",
      "[2400]\ttrain-rmse:0.18465\tval-rmse:0.71692\n",
      "[2600]\ttrain-rmse:0.16634\tval-rmse:0.71657\n",
      "[2800]\ttrain-rmse:0.15058\tval-rmse:0.71639\n",
      "[3000]\ttrain-rmse:0.13639\tval-rmse:0.71611\n",
      "[3200]\ttrain-rmse:0.12412\tval-rmse:0.71584\n",
      "[3400]\ttrain-rmse:0.11342\tval-rmse:0.71572\n",
      "[3600]\ttrain-rmse:0.10365\tval-rmse:0.71554\n",
      "[3800]\ttrain-rmse:0.09512\tval-rmse:0.71537\n",
      "[4000]\ttrain-rmse:0.08752\tval-rmse:0.71522\n",
      "[4200]\ttrain-rmse:0.08095\tval-rmse:0.71510\n",
      "[4400]\ttrain-rmse:0.07536\tval-rmse:0.71507\n",
      "[4600]\ttrain-rmse:0.07041\tval-rmse:0.71496\n",
      "[4800]\ttrain-rmse:0.06627\tval-rmse:0.71490\n",
      "[4999]\ttrain-rmse:0.06256\tval-rmse:0.71487\n",
      "Training XGBoost fold 2\n",
      "[0]\ttrain-rmse:0.93731\tval-rmse:0.93759\n",
      "[200]\ttrain-rmse:0.65327\tval-rmse:0.74926\n",
      "[400]\ttrain-rmse:0.55625\tval-rmse:0.72911\n",
      "[600]\ttrain-rmse:0.48795\tval-rmse:0.71985\n",
      "[800]\ttrain-rmse:0.43442\tval-rmse:0.71468\n",
      "[1000]\ttrain-rmse:0.38855\tval-rmse:0.71117\n",
      "[1200]\ttrain-rmse:0.34820\tval-rmse:0.70867\n",
      "[1400]\ttrain-rmse:0.31235\tval-rmse:0.70680\n",
      "[1600]\ttrain-rmse:0.28051\tval-rmse:0.70571\n",
      "[1800]\ttrain-rmse:0.25252\tval-rmse:0.70474\n",
      "[2000]\ttrain-rmse:0.22665\tval-rmse:0.70386\n",
      "[2200]\ttrain-rmse:0.20426\tval-rmse:0.70328\n",
      "[2400]\ttrain-rmse:0.18373\tval-rmse:0.70298\n",
      "[2600]\ttrain-rmse:0.16569\tval-rmse:0.70258\n",
      "[2800]\ttrain-rmse:0.14919\tval-rmse:0.70220\n",
      "[3000]\ttrain-rmse:0.13533\tval-rmse:0.70187\n",
      "[3200]\ttrain-rmse:0.12290\tval-rmse:0.70164\n",
      "[3400]\ttrain-rmse:0.11223\tval-rmse:0.70141\n",
      "[3600]\ttrain-rmse:0.10262\tval-rmse:0.70129\n",
      "[3800]\ttrain-rmse:0.09415\tval-rmse:0.70124\n",
      "[4000]\ttrain-rmse:0.08691\tval-rmse:0.70121\n",
      "[4018]\ttrain-rmse:0.08635\tval-rmse:0.70121\n",
      "Training XGBoost fold 3\n",
      "[0]\ttrain-rmse:0.93750\tval-rmse:0.93649\n",
      "[200]\ttrain-rmse:0.65500\tval-rmse:0.74685\n",
      "[400]\ttrain-rmse:0.56001\tval-rmse:0.72650\n",
      "[600]\ttrain-rmse:0.49339\tval-rmse:0.71799\n",
      "[800]\ttrain-rmse:0.43945\tval-rmse:0.71289\n",
      "[1000]\ttrain-rmse:0.39438\tval-rmse:0.70979\n",
      "[1200]\ttrain-rmse:0.35255\tval-rmse:0.70766\n",
      "[1400]\ttrain-rmse:0.31620\tval-rmse:0.70587\n",
      "[1600]\ttrain-rmse:0.28368\tval-rmse:0.70472\n",
      "[1800]\ttrain-rmse:0.25437\tval-rmse:0.70365\n",
      "[2000]\ttrain-rmse:0.22850\tval-rmse:0.70294\n",
      "[2200]\ttrain-rmse:0.20450\tval-rmse:0.70226\n",
      "[2400]\ttrain-rmse:0.18431\tval-rmse:0.70184\n",
      "[2600]\ttrain-rmse:0.16630\tval-rmse:0.70147\n",
      "[2800]\ttrain-rmse:0.14989\tval-rmse:0.70114\n",
      "[3000]\ttrain-rmse:0.13525\tval-rmse:0.70093\n",
      "[3200]\ttrain-rmse:0.12270\tval-rmse:0.70067\n",
      "[3400]\ttrain-rmse:0.11153\tval-rmse:0.70051\n",
      "[3600]\ttrain-rmse:0.10190\tval-rmse:0.70038\n",
      "[3800]\ttrain-rmse:0.09314\tval-rmse:0.70037\n",
      "[3854]\ttrain-rmse:0.09121\tval-rmse:0.70039\n",
      "Training XGBoost fold 4\n",
      "[0]\ttrain-rmse:0.93957\tval-rmse:0.92834\n",
      "[200]\ttrain-rmse:0.65667\tval-rmse:0.73792\n",
      "[400]\ttrain-rmse:0.55880\tval-rmse:0.71860\n",
      "[600]\ttrain-rmse:0.48923\tval-rmse:0.71021\n",
      "[800]\ttrain-rmse:0.43445\tval-rmse:0.70559\n",
      "[1000]\ttrain-rmse:0.38842\tval-rmse:0.70270\n",
      "[1200]\ttrain-rmse:0.34803\tval-rmse:0.70089\n",
      "[1400]\ttrain-rmse:0.31208\tval-rmse:0.69945\n",
      "[1600]\ttrain-rmse:0.28036\tval-rmse:0.69816\n",
      "[1800]\ttrain-rmse:0.25175\tval-rmse:0.69721\n",
      "[2000]\ttrain-rmse:0.22634\tval-rmse:0.69645\n",
      "[2200]\ttrain-rmse:0.20380\tval-rmse:0.69597\n",
      "[2400]\ttrain-rmse:0.18393\tval-rmse:0.69551\n",
      "[2600]\ttrain-rmse:0.16616\tval-rmse:0.69521\n",
      "[2800]\ttrain-rmse:0.15007\tval-rmse:0.69492\n",
      "[3000]\ttrain-rmse:0.13560\tval-rmse:0.69468\n",
      "[3200]\ttrain-rmse:0.12338\tval-rmse:0.69453\n",
      "[3400]\ttrain-rmse:0.11251\tval-rmse:0.69443\n",
      "[3600]\ttrain-rmse:0.10296\tval-rmse:0.69441\n",
      "[3800]\ttrain-rmse:0.09477\tval-rmse:0.69433\n",
      "[3930]\ttrain-rmse:0.08969\tval-rmse:0.69429\n",
      "Training XGBoost fold 5\n",
      "[0]\ttrain-rmse:0.93714\tval-rmse:0.93824\n",
      "[200]\ttrain-rmse:0.65262\tval-rmse:0.75427\n",
      "[400]\ttrain-rmse:0.55680\tval-rmse:0.73567\n",
      "[600]\ttrain-rmse:0.48987\tval-rmse:0.72747\n",
      "[800]\ttrain-rmse:0.43745\tval-rmse:0.72226\n",
      "[1000]\ttrain-rmse:0.39112\tval-rmse:0.71885\n",
      "[1200]\ttrain-rmse:0.35170\tval-rmse:0.71663\n",
      "[1400]\ttrain-rmse:0.31638\tval-rmse:0.71500\n",
      "[1600]\ttrain-rmse:0.28473\tval-rmse:0.71373\n",
      "[1800]\ttrain-rmse:0.25529\tval-rmse:0.71264\n",
      "[2000]\ttrain-rmse:0.22914\tval-rmse:0.71197\n",
      "[2200]\ttrain-rmse:0.20613\tval-rmse:0.71151\n",
      "[2400]\ttrain-rmse:0.18522\tval-rmse:0.71078\n",
      "[2600]\ttrain-rmse:0.16694\tval-rmse:0.71049\n",
      "[2800]\ttrain-rmse:0.15092\tval-rmse:0.71029\n",
      "[3000]\ttrain-rmse:0.13642\tval-rmse:0.71010\n",
      "[3200]\ttrain-rmse:0.12409\tval-rmse:0.70992\n",
      "[3400]\ttrain-rmse:0.11303\tval-rmse:0.70978\n",
      "[3600]\ttrain-rmse:0.10276\tval-rmse:0.70970\n",
      "[3800]\ttrain-rmse:0.09449\tval-rmse:0.70961\n",
      "[4000]\ttrain-rmse:0.08688\tval-rmse:0.70949\n",
      "[4200]\ttrain-rmse:0.08048\tval-rmse:0.70940\n",
      "[4400]\ttrain-rmse:0.07498\tval-rmse:0.70934\n",
      "[4600]\ttrain-rmse:0.07010\tval-rmse:0.70929\n",
      "[4800]\ttrain-rmse:0.06598\tval-rmse:0.70924\n",
      "[4876]\ttrain-rmse:0.06452\tval-rmse:0.70923\n",
      "Submission file created successfully: submission_optimized1.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# XGBoost Training (Universal Compatible)\n",
    "# -----------------------------\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "pred_xgb = np.zeros(X_test.shape[0])\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Training XGBoost fold {fold+1}\")\n",
    "\n",
    "    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=5000,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200\n",
    "    )\n",
    "\n",
    "    pred_xgb += model.predict(dtest, iteration_range=(0, model.best_iteration)) / kf.n_splits\n",
    "\n",
    "# -----------------------------\n",
    "# Ensemble\n",
    "# -----------------------------\n",
    "test_ensemble = 0.5 * pred_lgb + 0.35 * pred_cat + 0.15 * pred_xgb\n",
    "test_preds = np.expm1(test_ensemble)\n",
    "test_preds = np.clip(test_preds, train_df['price'].min(), train_df['price'].max())\n",
    "\n",
    "# -----------------------------\n",
    "# Create Submission\n",
    "# -----------------------------\n",
    "submission = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': test_preds})\n",
    "submission.to_csv(\"submission_optimized1.csv\", index=False)\n",
    "print(\"Submission file created successfully: submission_optimized1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e970131-314f-4499-ac51-03a263e4cb40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172bb4af-dc03-4a37-bf4e-ee42c7fb5be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
